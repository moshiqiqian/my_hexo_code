---
title: 霍普菲尔德网络
tags:

  - 霍普菲尔德网络
  - 人工智能
categories:
  - 教学
  - 人工智能
mathjax: true
description: 霍普菲尔德网络的原理以及公式解释
abbrlink: cbdf0372
date: 2025-05-24 18:21:08
---
## 霍普菲尔德（Hopfield Network）网络

### 背景介绍

Hopfield网络，由美国物理学家约翰·霍普菲尔德（John Hopfield）于1982年提出，它是早期人工神经网络的重要代表，对后续神经网络的发展产生了深远影响。

约翰·霍普菲尔德他的研究领域很广阔，包括物理学、生物分子学、神经科学，他的个人荣誉也很广阔，包括美国科学院院士、美国文理学院院士、美国哲学学会会士、美国物理学会会长等。

神经网络的研究最早可以追溯到由麦卡洛克（McCulloch），皮茨（Pitts）在1943年提出的麦卡洛克-皮茨模型（McCulloch-Pitts Model，简称MP模型），MP模型虽然展示了神经元可以通过简单的数学运算实现逻辑功能，但它缺乏学习机制，无法通过训练调整权重。

在1958年，美国科学家弗兰克·罗森布拉特（Frank Rosenblatt）在MP模型的基础上提出了感知机模型。但是当时的感知机还是太简单了，例如：单层的感知机还无法之下异或运算，多层感知机则类似一个黑盒，很多东西都不透明，模型的解释性不强，参数过多容易出错，过度拟合无法保证出现全局最优解等等问题的存在。这使得它没有地方可以很好的运用，因此神经网络的研究进入了长达几十年的低潮期。

神经网络研究的低潮在1982年Hopfield网络的提出之后，被彻底打破！Hopfield网络在联想记忆、数据恢复、优化问题求解等地方有了实际的应用价值。而且它为神经网络的研究提供了新的视角和方法，推动了神经网络技术从简单的线性模型向复杂的非线性动态系统发展。霍普菲尔德网络的成功也体现了跨学科研究的力量，为后续的人工智能和神经科学的发展开辟了新的道路。

### 基本原理引入

霍普菲尔德网络是一种联想记忆网络，它通过网络的动态演化（神经元状态迭代更新）收敛到稳定状态，将存储的模式作为这些稳定状态来实现模式的存储和回忆。

这样可能很笼统很难理解，下面我会具体直观的讲解。

先来从他的作用说起，简单来说，霍普菲尔德网络是一种查询方式，在无需逐个检查的方式下将，现有记忆和新输入之间建立联系。这看似不可能，但我们能从看似无关的生物学领域汲取灵感。

蛋白质是由氨基酸长链折叠成特定的三维结构的，蛋白质在三维空间中的排列方式难以计数，有研究证明，即使蛋白质在纳秒尺度采样不同的构想，达到正确配置的时间仍然超过宇宙年龄的时间。但实际上通常在毫米级别的时间就能快速的完成折叠。

有些人可能不太理解，会提问道：“蛋白质折叠成特定形状为什么不是蛋白质记忆了这种形状，直接按照特定形状折叠呢？”

将蛋白质折叠联想到“记忆”是很自然的，因为它确实展现出一种非随机的、有方向性的特性，似乎“知道”如何快速找到正确的终点，但是蛋白质的折叠不用记忆来描述。先来解释记忆：记忆是需要储存的，然后在需要的时候检索，但是蛋白质的折叠，没有检索这一个过程。

实际上，蛋白质的折叠只是落入最稳定有力的配置，就好想，石头从山谷滚落落入谷底的状态。让我们引入能量的概念，对于蛋白质，我们将对蛋白质链中原子相互作用储存的势能做研究。蛋白质的每种形态都会有不同的势能水平，这由这些原子相互作用的总和决定。我们可以抽象的将其视为只有二维，那么能量函数可以视为一个平面，每一个点，都代表一个蛋白质。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.7sng1qpi4j.webp)

点的海拔代表该配置的势能，这就是所谓的蛋白质**能量景观**。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.8adhqbqz11.webp)

蛋白质分子，倾向于最小化他的势能。受热力学第二定律的指导，自然的寻求尽可能低的能量水平，因为这代表着其原子的最佳排列。在折叠的过程中，它本质上是在能量景观上面，沿最陡路径向山谷滑落。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.54xzrdwnw5.webp)

这就是蛋白质为什么折叠这么快的原因，它不需遍历所有的情况，他们只需要沿着物理系统的自然趋势，将其势能最小化，就能得到目标结构。现在让我们将这种思想关联到记忆，来实现类似的效果。

假设我们有一个系统，它可以通过**T**[^1]与其**状态**[^2]之间的相互作用来编码信息，并且每种配置都具有特定的势能。首先我们需要创建能量景观，来将我们想要储存的记忆或者状态模式对应能量表面的中的最小值。其次我们需要有扮演热力学第二定律作用的东西，来驱动状态的变化，引导系统趋向局部最小值。即实现这两个目标，检索与模式最相近的记忆是通过，配置系统对最初输入模式编码，并让他由于运行到平衡状态，达到**能量井**[^3]，我们就可以从中读出原记忆。接下来让我们探索如何构建这种模式。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.13m0czsogm.webp)

###  Hopfield 网络架构

#### 构建能量景观

考虑一组神经元，他们可以为“+1”，“-1”两种状态，每一个神经元都和其他的神经元连接（全连接），他们中间都有相对应的权重，这里只考虑对称的权重（即神经元i到神经元j的权重和神经元j到神经元i的权重相等），之后在来解释原因。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.64e34jzndg.webp)

我们先来解释一下权重（W<sub>ij</sub>）的概念。如果W<sub>ij</sub>>0 那么两个神经元具有兴奋性。例如W<sub>ij</sub>是一个正数，那么意味着神经元i和j都紧密耦合，一个神经元会激发另一个神经元。在这种情况下，一个神经元处于激活状态，那么另一个神经元也倾向于激活状态，同样，一个神经元处于抑制状态，那么另一个神经元也倾向于抑制状态，即**促进它们状态上的一致性**。相反，如果W<sub>ij</sub><0，那么神经元 i 和神经元 j 之间的连接会**促进它们状态上的不一致性**。

如果连接权重 W<sub>ij</sub>期望神经元 i 和 j 的状态相同（即  W<sub>ij</sub>>0），但实际上它们的当前状态是相反的（s<sub>i</sub>s<sub>j</sub>=−1），那么这就产生了**冲突**。相反，如果 Wij 期望它们状态相反（即 W<sub>ij</sub><0），但实际上它们的当前状态是相同的（sisj=+1），这也产生了**冲突**。

将这个概念拓展到整个网络，网络中**所有神经元对之间**的冲突累加起来的总量定义为总冲突，这个总量越大，说明网络当前的状态离它“想要”的记忆模式越远，或者说，网络越不稳定。而冲突越少，连接权重和神经元的成对状态之间的整体一致性越高，即距离我们“想要”的记忆模式越近。

我们可以总结为：W<sub>ij</sub>x<sub>i</sub>x<sub>j</sub>越大（越接近正无穷）表明当前神经元对的状态与网络所存储的记忆模式之间的契合度越高，从而使系统更接近其稳定态（目标结果）。我们同样将这个概念拓展到整个网络，我们要寻求的是最大的“$\sum_{i \neq j} W_{ij} s_i s_j$”。那么显而易见，我们可以转换成求最小的“-$\sum_{i \neq j} W_{ij} s_i s_j$”。我们将其（$\sum_{i \neq j} W_{ij} s_i s_j$)定义为**构建能量景观的能量**，以此来构建能量景观。

![image](https://moshiqiqian.github.io/picx-images-hosting/霍普菲尔德网络/image.5q7ndorg52.webp)

#### 构建热力学第二定律的模式

由上文可知，我们希望霍普菲尔德网络能逐渐向能量最小值演化，但是观察公式，我们可以观察出，能量的大小取决于神经元的状态和权重两个部分。

我们先来解释一下权重在霍普菲尔德网络中代表着什么：霍普菲尔德网络的工作原理是将要存储的**记忆模式（或称为原型模式）**编码到这些连接权重中。通常，这是通过**赫布学习规则（Hebbian learning rule）**的变体来实现的。在霍普菲尔德网络中的具体体现为。

- 如果一对神经元 i 和 j 在许多被存储的模式中都处于**相同**的状态（比如在模式A中都是+1，在模式B中都是-1），那么它们之间的权重 Wij 就会被加强为**正值**。
- 如果它们在许多被存储的模式中都处于**相反**的状态（比如在模式A中是+1/-1，在模式B中是-1/+1），那么它们之间的权重 Wij 就会被加强为**负值**。

整个权重矩阵 W 实际上就包含了网络所有**学习到的、期望的**神经元状态配置，这些配置对应着能量景观中的局部最小值，也就是网络的记忆。

假设我们已经通过学习确定了权重的大小，但是神经元的静默状态是完全未知的，那么问题变成了，如何找到可以最小化整个网络的能量的状态模式。我们当然不会遍历所有的情况，我们会从其中一个记忆的部分或者噪声版本开始，或者是完全随机的配置，随后我们有两种方式来找到最小能量，同步更新和异步更新。

异步更新：在**异步更新**模式下，在每个时间步，**只选择一个神经元进行更新**。我们可以随机的选择或者顺序的选择要更新的神经元，把更新的神经元设置为“i”，我们将会计算**所有其他神经元对神经元 i 的加权影响之和**$\sum_{i \neq j} W_{ij} s_i s_j$，将这个结果表示为h<sub>i</sub>（净输入）。如果h<sub>i</sub>>0那么表示其他神经元赞成这个h<sub>i</sub>为“+1”，反之，如果h<sub>i</sub><0，那么，其他神经元赞成这个h<sub>i</sub>为“-1“。如果h<sub>i</sub>=0，会保持当前状态或者随机选择状态。

同步更新：在**同步更新**模式下，在每个时间步，**所有神经元都根据它们在上一时刻的状态同时计算净输入，并同时更新自己的状态**。更新规则与异步更新一致。

最终我们会到达一种配置，反转任何神经元都会导致，整体能量的上升。这时候我们认为网络已经收敛到稳定状态，也就是整体能量的最小值。我们用这种更新模式，来代替热力学第二定律。

***注释***：尽管同步更新在并行计算中看似更高效，但在霍普菲尔德网络的理论分析和实际应用中，**异步更新更为常见和推荐**。这是因为它能够**严格保证网络的收敛性**，确保网络总会找到一个稳定的记忆模式，而不会陷入无限循环或不可预测的振荡。这种确定性的收敛行为对于联想记忆的任务至关重要。

### 公式解释

$E = -\frac{1}{2} \sum_{i \neq j} W_{ij} s_i s_j$ 这是霍普菲尔德网络的能量公式，让我们来解释以下几个问题。

#### $-\frac{1}{2}$的由来

- 抵消重复计数

$- \sum_{i \neq j} W_{ij} s_i s_j$ 这个这部分的含义在上文中讲解过，不多做赘述，其中W<sub>ij</sub>代表权重，si和sj代表神经状态，$\sum_{i \neq j} $，表示我们对所有**不同的**神经元对 (i,j) 进行求和，且 i不等于j。例如：我们在求和的时候，分别加了$W_{ij} s_i s_j$ 和 $W_{ji} s_j s_i$，换句话说，求和 $\sum_{i \neq j} $实际上会把每对神经元之间的相互作用计算**两次**：一次是 (i,j)，另一次是 (j,i)。这两项的结果是完全相同的，为了对每一个神经元都只作用一次，所以我们要乘1/2来抵消这个重复的计数。

- 能量函数中 $-\frac{1}{2}$ 的存在使数学推导变得更加简洁和优雅

在异步更新中，我们**随机选择一个神经元 k**，并假设只有它的状态从 $s_k$变为 $s_k'$，而其他所有神经元 $j \neq k$ 的状态 $s_j$ 保持不变。

能量变化 $\Delta E$ 可以表示为新能量 $E'$ 减去旧能量 $E$：


$$\Delta E = E' - E$$

我们来分析能量函数中哪些项会发生变化。只有包含 $s_k$ 的项会改变。

旧能量 E 为：

$$E = -\frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k s_j + \sum_{i \neq k} W_{ik} s_i s_k + \sum_{i \neq k, j \neq k} W_{ij} s_i s_j \right)$$


新能量 E' 为：


$$E' = -\frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k' s_j + \sum_{i \neq k} W_{ik} s_i s_k' + \sum_{i \neq k, j \neq k} W_{ij} s_i s_j \right)$$


注意，最后一个求和项 $\sum_{i \neq k, j \neq k} W_{ij} s_i s_j$在 $E$ 和 $E'$ 中是相同的，因为它不包含 $s_k$。所以它在相减时会被抵消。

那么 $\Delta E$ 就简化为：


$$\Delta E = -\frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k' s_j + \sum_{i \neq k} W_{ik} s_i s_k' \right) - \left( -\frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k s_j + \sum_{i \neq k} W_{ik} s_i s_k \right) \right)$$


由于权重的**对称性 $W_{ik} = W_{ki}$**，我们可以将第二个求和项中的 $W_{ik} s_i s_k$ 改写为 $W_{ki} s_i s_k$。 所以：

$$\Delta E = -\frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k' s_j + \sum_{j \neq k} W_{kj} s_j s_k' \right) + \frac{1}{2} \left( \sum_{j \neq k} W_{kj} s_k s_j + \sum_{j \neq k} W_{kj} s_j s_k \right)$$


合并相同的项：


$$\Delta E = -\frac{1}{2} \left( 2 \sum_{j \neq k} W_{kj} s_k' s_j \right) + \frac{1}{2} \left( 2 \sum_{j \neq k} W_{kj} s_k s_j \right)$$


这里的 $-\frac{1}{2}$ 和 2 相乘，就变成了 -1 和 +1。


$$\Delta E = - \sum_{j \neq k} W_{kj} s_k' s_j + \sum_{j \neq k} W_{kj} s_k s_j$$


整理得到：

$$\Delta E = \sum_{j \neq k} W_{kj} s_j s_k - \sum_{j \neq k} W_{kj} s_k' s_j$$
$$\Delta E = \left( \sum_{j \neq k} W_{kj} s_j \right) (s_k - s_k')$$


我们定义神经元 k 的净输入为 $h_k = \sum_{j \neq k} W_{kj} s_j$。 所以，最终得到：


$$\Delta E = h_k (s_k - s_k')$$

如果没有$-\frac{1}{2}$ 那么最终的结果是：

$\Delta E^* = 2 h_k (s_k - s_k')$

 有了$-\frac{1}{2}$，能量变化 $\Delta E$ 的表达式**直接地**、**简洁地**呈现为 $h_k (s_k - s_k')$。这使得能量变化与神经元 k 的净输入 $h_k$ 和其状态变化 $(s_k - s_k')$ 之间存在一对一的、没有额外常数因子的关系。

#### 为什么权重要对称

保持权重的对称性其实主要是为了避免霍普菲尔德网络陷入周期性振荡，而不是收敛到单一稳定模式。

这其中涉及到的是**能量函数（或李亚普诺夫函数）的存在性**。简单的理解能量函数，你可以认为能量的**最低点就是稳定点**（正定性），而整个系统，在演化过程中，总会保持**能量的不变或减少**（负半定性）~~完美符合蛋白质的折叠过程不是吗？~~，这两个性质在霍普菲尔德网络中其中决定性的作用。

在**异步更新**且**权重对称**的情况下，霍普菲尔德定义的能量函数 $E = -\frac{1}{2} \sum_{i \neq j} W_{ij} s_i s_j$ 就**符合**了李亚普诺夫函数的关键性质。

由于满足了这些条件，我们就可以确信，霍普菲尔德网络在**回忆阶段**总会收敛到一个**能量的局部最小值**，而不会陷入无限循环或混沌状态。这些局部最小值正是网络预先存储的记忆模式。

本文参照：[A Brain-Inspired Algorithm For Memory](https://www.youtube.com/watch?v=1WPJdAW-sFo)



[^1]: 连接权重
[^2]: 指每个神经元是处于 +1 还是 -1 的“开”或“关”状态
[^3]: “能量井”是霍普菲尔德网络中**存储记忆的稳定状态**的视觉化比喻