---
title: 玻尔兹曼机
tags:
  - 玻尔兹曼机
  - 人工智能
categories:
  - 教学
  - 人工智能
  - 玻尔兹曼机
swiper_index: 0
description: 玻尔兹曼机的基础概念以及原理介绍
abbrlink: bb733ff7
date: 2025-07-18 00:44:15

---

## 玻尔兹曼机

### 前言

在历史的大部分时间里，计算机被视为纯逻辑机器，机械的处理数字，来产生明确的解决方案。毕竟没人希望在计算火箭升上太空的轨迹的时候，计算机即兴发挥，想出一些古怪的公式。但是到了2024年，各种创造式的人工智能层出不穷，那么是什么引发了这种转变？什么时候双层网络超越了确定性计算？

认识一下玻尔兹曼机吧，一种敢于拥抱混乱，并且永远改变人工智能的神经网络！

20世纪80年代，玻尔兹曼机引入了一个激进的概念，如果我们把不确定性和随机性融入机器学习的结构中。如果我们的AI能够掌握支配我们周围世界的潜在概率规则，而不是存储僵化的事实和执行确定性计算，会发生什么情况？在了解玻尔兹曼机之前，我强烈建议你先了解一下玻尔兹曼机的前身，霍普菲尔德网络，其中的很多在霍普菲尔德网络中介绍的概念和思想将不再赘述。可以参照文章：[霍普菲尔德网络 | moshiqiqian](https://www.moshiqiqian.top/post/cbdf0372.html)。

简而言之，霍普菲尔德网络是一种联想记忆模型，它拥有从部分或者嘈杂的输入中会议完整模式的能力，他通过为每种可能状态分配一个特定能量，构建一个能量景观，然后通过沿能量表面下降到能量井，从而快速的回忆起来最匹配的记忆存储模式。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.45z5but32.webp)

霍普菲尔德网络显然有优秀的记忆力，并且擅长去根据一些“蛛丝马迹”来回忆“记忆”。但霍普菲尔德网络回忆的能力仅限于再现它明确学到的东西，它不能创建新的模式或理解它所看到的数据的底层结构。这就是波尔兹曼机发挥作用的地方，它提供了一种更灵活、更有创意的信息处理方法。

举一个例子来对比他们两个，先来介绍霍普菲尔德网络，可以把它想象成一个音乐爱好者，他可以从几个初始音符中识别并完美地再现一首著名的杰作。而玻尔兹曼机，可以把它想象成一个艺术大家，不仅了解特定的歌曲，还参透了音乐本身固有的基本规则和结构。当给出几个开场音符时，这位音乐家不会简单地回忆和演奏现有的乐曲，而是利用对音乐理论的深刻理解，结合创造力来即兴创作。玻尔兹曼机不同于联想网络，它不仅记忆数据点，还学习数据的底层概率分布，捕捉构成事物的本质。

乍一看这两个系统似乎根本不同，在算法上几乎没有共同之处，但实际上它们非常密切相关，只需两个关键的技术修改就可以将任何霍普菲尔德，即**随机性**和**隐藏单元**。

### 基本原理引入

我们从 19 世纪的奥地利开始，当时一位年轻的物理学家 Ludwick Boltzman（路德维希·玻尔兹曼）正在努力解决一个基本问题，想象一个像气体这样的粒子系统，每个粒子都有自己的能量，由其速度等因素决定，我们可以通过测量温度在宏观尺度上测量粒子的平均能量。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.lw0twwdlm.webp)

但在单个粒子层面会发生什么？我们可以想象每一个粒子在精确的能量值方面可能有所不同。而玻尔兹曼探索的就是理解这种能量分布，换句话说，如果我们随机选择一个粒子，它具有特定能量值的概率是多少？玻尔兹曼通过指数关系将状态的概率与其能量联系起来，**一个状态 s 拥有能量 e 的概率，与负能量除以温度的指数成正比**。

简单来说这个公式的核心思想是：**在热平衡状态下，系统更倾向于占据能量较低的微观状态。**

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.2yynb4a912.webp)

为了理解这里为什么会出现指数，请将**能级**想象成楼梯上的台阶，粒子在他们之间跳跃，每一步都代表一个小的能量增量 $\epsilon$，对于一个粒子来说，要向上移动一步，它必须获得 $\epsilon$ 单位的能量，也许是通过与另一个粒子的碰撞，我们将这种碰撞的概率称为P，在大量分子中，这个概率本质上是常数，并且仅取决于平均粒子速度或温度，如果一个粒子以概率 P 跳上一级，它可能会立即以相同的概率再次跳跃，因为概率在独立事件中相乘，所以跳跃两级的概率是 P 平方，跳跃三级是 p 立方，依此类推，我们看到一个模式，跳跃 n 级的概率是 p 的 n 次方。现在考虑一个粒子增加 $\Delta E$的能量，它必须爬多少级？显而易见是$\frac{\Delta E}{\varepsilon}$级，因此跃迁到更高能量状态的概率是$p^{\frac{\Delta E}{\epsilon}}$。

我们可以将 P 的温度依赖性移入指数部分，并将底数改为 e（我们知道任何正数 x 都可以表示为  $e^{\ln(x)}$）。我们定义$-\frac{\ln[p]}{\epsilon} = T$。请注意，由于 P 根据概率的定义小于1，而 e 大于1，这使得在指数中能量之前必须有一个负号，因为温度总是正的。在教科书中，你通常会看到它的一个版本为$e^{-\frac{\Delta E}{kT}}$，在温度前面带有一个玻尔兹曼常数 k。但这个常数是用来将以开尔文度（或开尔文）测量的温度单位转换为以焦耳测量的能量单位。在这里我们将把玻尔兹曼常数并入（物理）单位中。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.51efz68wxa.webp)

这个方程给出了从一个状态转换到另一个状态的相对概率，但是我们如何才能找到特定能量状态的绝对概率呢？看以下这个例子。

假设我们的系统只能存在三种状态，能量值分别为 1、2 和 3，以任意单位测量，假设温度等于 1，这个方程告诉我们，找到系统处于状态二的可能性是找到处于能量较低的状态一的可能性的 $e^{-1}$倍，找到处于状态三的可能性是找到状态一的可能性的$e^{-2}$ 倍，这是他们的相对值。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.7sni78v45q.webp)

如何找到他们的绝对值呢？我们首先并不知道状态一的基线概率，那么我们如何才能找到它呢？这里的缺失环节是：所有绝对概率的总和必须为一。这样我们就可以用方程解出他们的绝对概率。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.3yeqoad9cz.webp)

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.361v6jwqdh.webp)

这表明，我们可以从由玻尔兹曼公式得出的能量增加的相对概率，通过求解包含所有可能状态的方程，得到绝对值。

让我们将绝对能量值代入指数公式，用E代替$\Delta E$（可以简单的理解为，一个状态的能量是 E 时，这个 E 实际上可以被看作是该状态与参考点（能量为 0）之间的 ΔE。）我们可以得到相对概率的图像，同样我们画出绝对概率的图像做对比。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.4cl6f5lph3.webp)

注意，一个形状看起来像另一个形状的垂直缩放版本。这是一个至关重要的见解，因为绝对概率必须与相对转移概率成正比。

一个能量为 E 的状态的绝对概率和相对概率就由常数因子 Z（$\frac{1}{Z}e^{-E/T}$）关联了起来。这个常数对应着适当的重新缩放。**Z 的值可以通过确保所有可能状态的概率之和为一来找到**。这个归一化因子被称为**配分函数（partition function）**。它考虑了所有可能的状态以及能量如何在它们之间分布。如下是他的推导过程以及公式。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.5mo3lh3r76.webp)

这就是玻尔兹曼分布的完整最终版本，它将能量与概率联系起来。要使用它，首先查看所有可能的状态，并且利用上述的推导公式，得到 Z 的值。然后，要找到系统处于某个特定能量状态的概率（绝对概率），先通过玻尔兹曼分布的公式计算该特定能量相对概率，并将其除以 Z，得到他的绝对概率。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.4xuu1ggaw7.webp)

### 更新规则（神经元更新）



**注意**：由于 {-1,1} 的表示更符合我们对神经元“激活”和“非激活”的直观理解，而不是 {0,1} ，所以本文更好理解所以设定为{-1,1}。

在任意给定的更新时刻，我们关注的是单个神经元（假设为神经元 i）的两种候选状态：“开”（on）或 “关”（off），而网络中的其他神经元状态则保持不变。网络的总能量被定义为权重和成对神经元状态之间的“冲突程度”。
$$
E_{\text{on}} = \overbrace{\sum_{j \neq i} -w_{ij} x_j}^{E(x_i = +1)} + E_{\text{rest}}
\quad \quad \quad \quad
E_{\text{off}} = \overbrace{\sum_{j \neq i} w_{ij} x_j}^{E(x_i = -1)} + E_{\text{rest}}
$$
这两个公式的第一项表示神经元I对总能量的贡献，而第二项表示网络其余部分贡献的能量，这部分能量不受神经元I的影响。

**注意**：
1.在霍普菲尔德网络中其实也含有E<sub>rest</sub>，但是我们通常不做讨论，虽然霍普菲尔德网络的总能量中也包含所有神经元的加权，并且也可以从理论上拆分出与 Erest 对应的部分，但在其确定性的更新机制和单调递减的能量特性下，**显式地分离出E<sub>rest</sub> 并没有像在玻尔兹曼机中那样带来独特的洞察力或计算上的简化（因为玻尔兹曼机的随机更新依赖于概率比，而 E<sub>rest</sub>在概率比中会消掉）**。霍普菲尔德网络的分析更侧重于能量的绝对下降和收敛到局部最小值。玻尔兹曼机由于其随机性，需要更细致地考虑局部能量项对概率的影响。

2.每个神经元都有一个“激活门槛”，即阈值。当它从其他神经元获得的输入总和超过这个门槛时，它才会被激活。
为了简化数学模型，我们引入了一个巧妙的技巧：可以想象有一个“永远开启”的虚拟神经元。这个虚拟神经元虽然没有物理实体，但它与网络中每个神经元都有连接。通过将这条连接的权重设为原来阈值的负值，我们就成功地把复杂的“阈值”概念，转化成了一个简单的“偏置”，并将其融入了网络本身的连接中。这样，所有需要调整的参数——无论是神经元之间的连接，还是神经元自身的偏好——都可以用统一的权重来表示，极大地简化了玻尔兹曼机的计算和学习过程。
具体解释可以去看玻尔兹曼机的文章。

以开启状态举例，我们可以用玻尔兹曼分布来表示神经元 I 开启的概率：玻尔兹曼机只有两种可能的状态（开或者关）

首先，我们定义神经元 i 处于两种不同状态时的总能量。我们已经知道，总能量 E 可以分为两部分：

1. **与神经元 i 相关的能量贡献 (E<sub>i</sub>)**：这部分取决于神经元 i 自身的状态和它与邻居的连接。
2. **网络其余部分的能量贡献 (E<sub>rest</sub>)**：这部分是常数，因为它不随神经元 i 的状态改变。

$$
E_{\text{on}} = \overbrace{\sum_{j \neq i} -w_{ij} x_j}^{E(x_i = +1)} + E_{\text{rest}}
$$



玻尔兹曼分布告诉我们，系统处于某个特定状态的概率与其能量的负指数成正比。对于神经元 i 处于“开”状态的概率 （**这里为了更好的理解公式我们设T=1**）：
$$
p_{\text{on}} = \frac{1}{Z}e^{-E_{\text{on}}}
$$


在只有两种状态（开和关）的情况下，Z 的定义如下：
$$
Z = e^{-E_{\text{on}}} + e^{-E_{\text{off}}}
$$


将Z带入p<sub>on</sub>：
$$
p_{\text{on}} = \frac{e^{-(E_i^{(+1)}+E_{\text{rest}})}}{e^{-(E_i^{(+1)}+E_{\text{rest}})}+e^{-(E_i^{(-1)}+E_{\text{rest}})}}
$$


利用指数的性质
得出：
$$
p_{\text{on}} = \frac{e^{-E_i^{(+1)}} \cdot e^{-E_{\text{rest}}}}{e^{-E_i^{(+1)}} \cdot e^{-E_{\text{rest}}} + e^{-E_i^{(-1)}} \cdot e^{-E_{\text{rest}}}}
$$
约掉共同项：
$$
p_{\text{on}} = \frac{e^{-E_i^{(+1)}}}{e^{-E_i^{(+1)}} + e^{-E_i^{(-1)}}}
$$
分子和分母同时除以分子 :
$$
p_{\text{on}} = \frac{1}{1+e^{-E_i^{(-1)}}/e^{-E_i^{(+1)}}}
$$
再次利用指数性质得出:
$$
p_{\text{on}} = \frac{1}{1+e^{-(E_i^{(-1)}-E_i^{(+1)})}}
$$
我们定义$\Delta E = E_i^{(-1)} - E_i^{(+1)}$ ，显而易见：$\Delta E = E_i^{(-1)} - E_i^{(+1)}$  = $2 \sum_{i \neq j} w_{ij}x_j$，代表了神经元 i 从“开”状态变为“关”状态时，局部能量的**变化**。

最终得到 **Sigmoid 函数**：这代表着开启状态下的绝对概率
$$
p_{\text{on}} = \frac{1}{1 + e^{-2 \sum w_{ij}x_j}}
$$
以下为推导过程

  ![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.99tn8zzsni.webp)

它告诉我们，当一个神经元的输入（$\sum w_{ij}x_j$）是正值时，该神经元更有可能切换到“开启”状态，并且输入越大，其概率也就越大。当输入为负时，开启的概率会下降，当加权输入为非常负的值时，开启的概率接近于零。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.5xaxemjasc.webp)

**注意：** 若神经元状态为{0，1}那么公式为如下：
$$
p_{\text{on}} = \frac{1}{1 + e^{- \sum w_{ij}x_j}}
$$

#### 总结

基于上述原理，玻尔兹曼机的随机**更新规则**总结如下（也就是所谓的吉布斯采样）：

1.**计算加权输入：** 首先，计算神经元 i 的所有输入连接的加权和。

2.**计算概率 P：** 使用前面提到的 Sigmoid 函数，将计算出的加权输入代入，得到神经元 i 变为“开”状态的概率 P。

3.**生成随机数：** 产生一个介于 0 和 1 之间的随机数。

4.**更新状态：**

- 如果生成的随机数小于概率 P，则将神经元 i 的状态设置为 +1（“开”）。
- 否则，将神经元 i 的状态设置为 -1（“关”）。

最后我们把默认为1的T带入公式，公式将变成：

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.83ac0eb1x3.webp)

T的存在允许神经元有时切换到更高能量状态,其概率取决于能量差和温度。在高温下，决策变得更加随机，而在低温下，它们接近霍普菲尔德网络的确定性行为。温度通常是一个超参数，我们可以根据模型的创造性来调整它。

例如（图中的net<sub>i</sub>代表加权能量和）：

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.77duky1fq3.webp)

这个随机规则对于玻尔兹曼机来说至关重要。它允许网络摆脱能量格局中的局部最小值并探索更广泛的状态，使其能够学习更复杂的概率分布并产生更多样化的输出。随机更新规则是玻尔兹曼机推理的关键修改。但你可能会想，这种随机性是否也会改变我们的学习方式?我们如何塑造能源格局？确实如此，正如我们很快就会看到的，它引出了一个令人着迷的概念，即对比学习规则。

### 对比学习规则（权重的确定）

玻尔兹曼机想要学习的是，数据的底层概率分布。理想情况下，当网络随机探索可能状态的景观时，我们希望它在与训练数据中的模式相更相似。换句话说，我们希望这些状态更精确。回想一下玻尔兹曼分布，它将概率与能量联系起来。根据这个公式：$p(S_E) = \frac{1}{Z}e^{-E/T}$，为了增加某个状态的概率，我们需要降低它相对于其他状态的能量。可以参照霍普菲尔德网络的思想，将能量最小值对应到最优解。

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.2a5dr3nrhh.webp)

让我们来一步步推导：

训练数据$D = \{\mathbf{x}^{(1)},...,\mathbf{x}^{(N)}\}$

**1. 目标：最大化训练数据的联合概率** 玻尔兹曼机学习的根本目标是使其能够很好地“解释”或“重现”训练数据。这意味着，模型应该为训练数据中的每个样本分配高概率。 由于训练样本之间通常被认为是独立的（或至少条件独立），它们的联合概率就是每个单独样本概率的乘积：
$$
P(\text{data}) = P(\mathbf{x}^{(1)},\mathbf{x}^{(2)},...,\mathbf{x}^{(N)}) = \prod_{n=1}^{N} P(\mathbf{x}^{(n)})
$$
**2. 转换为最大化对数似然** 为了简化优化问题，我们对联合概率取**对数**。由于对数是一个单调递增函数，最大化 P(data) 等价于最大化 logP(data)。
对数会将乘积转化为求和，这在数学上更容易处理：
$$
\log P(\text{data}) = \log \left(\prod_{n=1}^{N} P(\mathbf{x}^{(n)})\right) = \sum_{n=1}^{N} \log P(\mathbf{x}^{(n)})
$$
**3. 用玻尔兹曼分布和能量函数表示单样本概率** 现在，我们用玻尔兹曼分布来表示每个训练样本$\mathbf{x}^{(n)}$ 在模型下的概率。这里的$\mathbf{x}^{(n)}$就是一个特定的神经元状态配置，其能量为 E($\mathbf{x}^{(n)}$)。
$$
P(\mathbf{x}^{(n)}) = \frac{1}{Z}e^{-E(\mathbf{x}^{(n)})/T}
$$


将此代入对数似然函数：
$$
\log P(\text{data}) = \sum_{n=1}^{N} \log \left(\frac{1}{Z}e^{-E(\mathbf{x}^{(n)})/T}\right)
$$
↓ (利用对数性质展开)

**4. 展开对数似然函数** 利用对数的性质 $\log P(\text{data}) = \sum_{n=1}^{N} \left( \log(e^{-E(\mathbf{x}^{(n)})/T}) - \log Z \right)$，我们可以展开表达式： 
$$
\log P(\text{data}) = \sum_{n=1}^{N} \left( -\frac{E(\mathbf{x}^{(n)})}{T} - \log Z \right)
$$
**5. 进一步展开求和项** 我们将求和符号展开到内部的每一项：
$$
\log P(\text{data}) = -\frac{1}{T}\sum_{n=1}^{N} E(\mathbf{x}^{(n)}) - N \log Z
$$
为了最大化这个表达式，我们需要同时：

- **最小化 $\sum_{n=1}^{N} E(\mathbf{x}^{(n)}):$：** 因为这一项前面有一个负号和 1/T (通常 T>0)，所以为了最大化整个表达式，我们需要让所有训练模式的**能量之和尽可能小**。直观上，这意味着模型应该学习到，真实的训练数据是“低能量”的状态，它们是系统偏好的稳定状态。
- **最小化 NlogZ：** 由于 N 是训练样本的数量（正数），我们同样需要让 logZ 尽可能小。这意味着需要**最大化 Z**。
  - **理解 Z：** $Z = \sum_{\text{所有可能状态}} e^{-E(\mathbf{s})/T}$。Z 可以被看作是所有可能状态的“能量权重之和”，它反映了**模型能够生成的所有可能状态的“多样性”或“容量”**。
  - **最大化 Z 意味着什么？** 这通常意味着模型需要学习到，除了训练数据处于低能量状态，其他（非训练数据）的状态的能量也要保持在一个相对较高的水平，或者说，模型不能把所有的能量都集中在少数几个点上。这鼓励模型形成一个宽泛的能量盆地，能覆盖多种状态，而不仅仅是狭窄地记住训练数据。
  - **简单来说：我们的能量景观在谷底的时候是一个宽泛，圆润的形状，区别于霍普菲尔德网络陡峭的能量景观，这样我们在输出的时候，会有更多的多样性。**

它本质上创造了两种对立的力量。一种是在所需数据周围挖掘能量井，而另一种是将不需要的数据的能量表面向上拉。我们可以对给定的权重取对数概率的导数，然后对权重进行迭代调整以使其最大化。这是一个复杂冗长的推理，这里不再赘述。

#### 总结

总而言之，经过推导后，我们可以得到所谓的对比赫布学习规则：

![image](https://github.com/moshiqiqian/picx-images-hosting/raw/master/玻尔兹曼机/image.6ikl0xe9ps.webp)

第一项是网络接触训练数据时状态xi 和xj 的平均乘积（正项也称正阶段）：

- **含义：** 在**训练数据**驱动下，神经元 i 和 j 的状态乘积的平均值。
- **作用：** 类似赫布规则，它**加强**在真实数据中经常**共同激活**的神经元之间的连接，使模型更好地“记住”训练数据的模式。

第二项是这两个神经元的网络自由运行时的平均乘积（负项也称负阶段）：

- **含义：** 在模型自由运行（从模型自身分布中采样）时，神经元 i 和 j 的状态乘积的平均值。
- **作用：** 它**削弱**模型自身“幻觉”出的、但在真实数据中不常见的关联。这一项阻止模型将概率分配给不存在于训练数据中的虚假模式，并确保配分函数 Z 的正确性。

**注意：**⟨x<sub>i</sub> x<sub>j</sub>⟩ 指代**由权重 w<sub>ij</sub> 连接的任意两个神经元 x<sub>i</sub> 和 x<sub>j</sub>** 的状态乘积的平均值。

确定具体的$\Delta w_{ij}$，我们需要引入学习率（通常是一个很小的正数，比如 0.01 或 0.001）。学习率决定了我们沿着梯度方向**迈出的步子有多大**。
$$
\Delta w_{ij} = \eta \cdot \left( \left\langle x_{i} x_{j} \right\rangle_{\text{data}} - \left\langle x_{i} x_{j} \right\rangle_{\text{model}} \right)
$$


相比于霍普菲尔德网络，有一个明确的公式来表示作为训练模式函数的权重。玻尔兹曼机的一个主要区别是学习不再是即时的。相反，它涉及一个迭代过程，这分为两个阶段。正阶段、负阶段。



### 具体的权重更新过程

为了详细的介绍他的具体学习过程我们要引入一个概念，可见单元和隐藏单元。

本质上隐藏单元就是：**不直接对应于输入或输出的任何部分的神经元。** 他们用于捕捉数据中可见单元中无法立即显现的抽象特征和高阶相关性。

实现隐藏单元很简单，只是增加了网络中神经元的数量。将一些指定为可见，将另一些指定为隐藏。可见单元的数量通常与数据的维数相对应。例如，32 x 32 像素的图像需要 1024 个可见神经元，每个像素一个。然而，隐藏单元的数量是一种设计选择，可以任意多。重要的是，虽然可见单元和隐藏单元之间存在概念区别，但网络在更新规则方面对它们采取相同的处理方式。它计算加权输入并一次对一个神经元执行随机更新，无论其类型如何。

#### 标准玻尔兹曼机

在标准玻尔兹曼机下，我们将所有的单元分为可见单元和隐藏单元，所有的单元都是全连接，所以我们不会以可见层和隐藏层来区分它们，因为他们并没有具备明显的分层现象。

- 正阶段

1.  可见单元 v 的状态被**钳制 (clamped)** 为训练数据的值。这是**确定性的**。
2.  依据**更新规则**确定隐藏单元的值（第一次正阶段的权重是随机的，随后的权重是上一次更新的权重）。
3.  记录在达到条件平衡后，**所有单元配对活动的平均值**$\left\langle x_{i} x_{j} \right\rangle_{\text{data}}$。这称为**正梯度项**。

- 负阶段

1. 不向可见单元输入数据，自由采样（**权重**使用的是**当前迭代周期开始时的权重**，与正阶段**相同**）。
2. 记录在达到全局平衡后，**所有单元配对活动的平均值**$\left\langle x_{i} x_{j} \right\rangle_{\text{model}}$。这称为**负梯度项**。

**注意：** 标准玻尔兹曼机下，所有单元的平均值，不仅仅为<可见，隐藏>,还包含<可见，可见>,<隐藏，隐藏>。

- 更新权重：按照**对比学习规则**更新权重。



#### 受限玻尔兹曼机

标准玻尔兹曼机是全连接的，在收敛过程中数据量巨大，收敛十分缓慢，所以我们更喜欢使用简化后的受限玻尔兹曼机。受限玻尔兹曼机正式分为了可见层和隐藏层，同一层中神经元没有相互链接，这使得计算量大大减少。
以下介绍的是对比散度法，是最常见的，最基础的更新权重的方法。

- 正阶段 (Positive Phase)

1. **可见单元层的状态被钳制 (clamped) 为训练数据的值。** 这是确定性的。
2. 依据**更新规则**确定隐藏层的值（第一次正阶段的权重是随机的，随后的权重是上一次更新的权重）。
3. 记录在达到条件平衡后，**所有单元配对活动的平均值**$\left\langle x_{i} x_{j} \right\rangle_{\text{data}}$。这称为**正梯度项**。



- 负阶段 (Negative Phase)


1. 从正阶段更新后的隐藏层开始，反向重构可见层。
2. 再次依据**更新规则**确定隐藏层的值（权重使用的是当前周期的权重）。
3. 记录在达到全局平衡后，**所有单元配对活动的平均值**$\left\langle x_{i} x_{j} \right\rangle_{\text{model}}$。这称为**负梯度项**。



- 权重更新：按照**对比学习规则**更新权重。

**注意：** 标准玻尔兹曼机下，所有单元的平均值，仅为<可见，隐藏>。

本文参照：[Generative Model That Won 2024 Nobel Prize](https://www.youtube.com/watch?v=_bqa_I5hNAo&t=1320s)