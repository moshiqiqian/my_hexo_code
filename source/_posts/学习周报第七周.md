---
title: 学习周报第七周
tags:
  - 人工智能
categories:
  - 周报
  
description: 并行计算实验室学习周报
abbrlink: 8d942792
date: 2025-09-08 10:29:45
---
## 统计学上的qda，qdf学习周报

### 名词解释

**高斯分布**：又称**正态分布**或**常态分布**，高斯分布有以下几个核心特点：

- **对称性：** 曲线以其平均值为中心，左右完全对称。这意味着数据在平均值两侧的分布是均衡的。

- **单峰性：** 曲线只有一个峰值，位于平均值处，表示该点的概率密度最大。

- **钟形曲线：** 其概率密度函数曲线形状像一个钟，因此得名“钟形曲线”。

- **由两个参数决定：**

  - **均值 (μ)：** 决定了分布的中心位置，也就是钟形曲线的顶点所在的位置。
  - **标准差 (σ)：** 决定了分布的“胖瘦”或“扩散程度”。标准差越大，曲线越扁平，数据越分散；标准差越小，曲线越高越尖，数据越集中。

- **无限延伸：** 理论上，高斯分布的曲线从负无穷延伸到正无穷，但离均值越远，概率密度越接近于零。

  用数学公式表示，如果一个随机变量 X 服从均值为 μ、标准差为 σ 的高斯分布，则其概率密度函数为：
  $$
  f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
  $$


  当 μ=0 且 σ=1 时，我们称之为**标准正态分布**。

 **协方差矩阵**：

- 协方差：类似于霍普菲尔德网络权重之间的关系，分为正协方差，负协方差，零协方差。

  - **正协方差：** 如果两个变量倾向于同时增大或同时减小，它们的协方差为正。
  - **负协方差：** 如果一个变量增大时另一个变量倾向于减小，它们的协方差为负。
  - **零协方差：** 如果两个变量之间没有明显的线性关系，它们的协方差会接近零。

- 协方差矩阵：将协方差以矩阵的形式表示
  $$
  \Sigma = \begin{pmatrix}
  Cov(X_1, X_1) & Cov(X_1, X_2) & \cdots & Cov(X_1, X_n) \\
  Cov(X_2, X_1) & Cov(X_2, X_2) & \cdots & Cov(X_2, X_n) \\
  \vdots & \vdots & \ddots & \vdots \\
  Cov(X_n, X_1) & Cov(X_n, X_2) & \cdots & Cov(X_n, X_n)
  \end{pmatrix}
  $$

**注意**：**一个随机变量与自己本身的协方差就是它的方差**。

**协方差计算过程：**

**计算变量 X 的平均值 (μX)：** 将所有 X 值加起来，然后除以样本总数 m。
$$
\mu_X = \frac{1}{m} \sum_{i=1}^{m} x_i
$$


**计算变量 Y 的平均值 (μY)：** 将所有 Y 值加起来，然后除以样本总数 m。
$$
\mu_Y = \frac{1}{m} \sum_{i=1}^{m} y_i
$$

**偏差乘积之和**:这一步是协方差公式的分子部分。它计算了每个数据点与各自平均值的偏差的乘积，并将它们求和。
$$
\sum_{i=1}^{m} (x_i - \mu_X)(y_i - \mu_Y)
$$


**除以样本数减 1 (m−1)：** 将和除以 (m−1)。使用 m−1 是为了得到**无偏估计量**，这在统计推断中更常用。
$$
Cov(X, Y) = \frac{1}{m-1} \sum_{i=1}^{m} (x_i - \mu_X)(y_i - \mu_Y)
$$


### QDA

二次判别分析，是一种统计分类方法，它假设每个类别的数据都服从一个**多元高斯分布**，允许每个类别拥有自己独特的协方差矩阵。

- 二次：指的是二次多项式。类别之间的边界是二次函数。这会让QDA更加灵活。
- 优点：灵活性高
- 缺点：数据集少的时候容易过度拟合。

**QDF**：二次判别函数，是一种用于**分类**的数学函数。

以身高体重为特征，男人，女人为分类举例：

得到的**协方差矩阵**为
$$
\Sigma = \begin{pmatrix}
Var(\text{身高}) & Cov(\text{身高}, \text{体重}) \\
Cov(\text{体重}, \text{身高}) & Var(\text{体重})
\end{pmatrix}
$$
注意：

1. 协方差矩阵与数据量无关，只与他的特征数量相关。
2. 协方差的取值依赖于特征的关系，例如：体重与身高成正比，那么协方差为正数。



有了这些信息，QDA 的分类过程大致分为以下步骤：

1.定义每个类别的概率分布

QDA 假设每个类别 k 的数据都服从一个多元高斯分布 N(μk,Σk)。

- **μk (均值向量):** 代表类别 k 的“中心”位置。
- **Σk (协方差矩阵):** 描述类别 k 数据的“形状”和“方向”。比如，如果男性和女性在身高-体重上的分布是不同方向的椭圆形，Σ男性 和 Σ女性 就会捕捉到这些不同的椭圆形状。

有了这些，QDA 就可以计算一个新数据点 x 属于某个类别 k 的**概率密度** P(x∣k)

2.利用贝叶斯定理构建判别函数：
$$
P(k \mid X) = \frac{P(X \mid k) P(k)}{P(X)}
$$
其中：

- P(k∣x) ：在观察到数据 x 后，它属于类别 k 的概率。
- P(x∣k) 是数据 x 在类别 k 的概率密度（由类别 k 的高斯分布定义，其中就用到了 Σk）。
- P(k) 是类别 k 的**先验概率**（即在没有任何数据信息之前，类别 k 出现的概率，通常由训练集中各类别所占比例估算）。
- P(x) 是数据 x 出现的总概率密度，对于所有类别都是一样的，所以在比较不同类别时可以忽略。

为了方便计算和比较，QDA 通常会使用**对数后验概率**来构建判别函数。当你取对数并简化后，就会得到每个类别的**二次判别函数（ QDF）** gk(x)：
$$
g_k(X) = -\frac{1}{2} \log|\Sigma_k| - \frac{1}{2}(K - \mu_k)^T \Sigma_k^{-1} (K - \mu_k) + \log(\pi_k)
$$
3.分类决策：选择得分最高的类别

当一个新的、未标记的数据点 x<sub>new</sub> 到来时，QDA 会：

1. **计算所有类别的判别函数值：** 将  x<sub>new</sub>  代入每一个类别的 g<sub>k</sub>(x) 函数，得到 g1( x<sub>new</sub> ),g2( x<sub>new</sub> ),…,gC( x<sub>new</sub> ) （假设有 C 个类别）。
2. **分配类别：** 将  x<sub>new</sub> 分配给使其判别函数值 **gk( x<sub>new</sub> ) 最大的那个类别**。

4.形成非线性决策边界:

由于每个类别的协方差矩阵是独立的，它们各自的判别函数 g<sub>k</sub>(x) 会有所不同。当 g<sub>i</sub>(x)=g<sub>j</sub>(x) 时，就形成了类别 i 和类别 j 之间的**决策边界**。因为这些判别函数是**二次**的，所以这些决策边界将是**非线性的**，可以是曲线、椭圆、双曲线等。

### 受限玻尔兹曼机

受限玻尔兹曼机（Restricted Boltzmann Machine，简称 RBM）是**玻尔兹曼机的一种特殊形式**，也是深度学习领域中一个非常重要的基石模型。它由 Geoffrey Hinton 发明，因其结构上的“限制”而得名，正是这些限制使得 RBM 比通用的玻尔兹曼机更易于训练和应用。

**特点**：分为两层可见层和隐藏层，但是同一层之间的神经元没有连接，取而代之，可见层和隐藏层之间进行全连接。

- 可见层：负责接收输入数据。
- 隐藏层：负责学习数据的高阶抽象特征。

**优点**：在保留玻尔兹曼机的属性的情况下，我们极大的简化了模型的复杂性，增加了计算的效率。

**能量函数**：
$$
E(v, h) = -\sum_{i} a_i v_i - \sum_{j} b_j h_j - \sum_{i,j} v_i h_j w_{ij}
$$

- vi 是可见层神经元 i 的状态。
- hj 是隐藏层神经元 j 的状态。
- ai 是可见层神经元 i 的偏置。
- bj 是隐藏层神经元 j 的偏置。
- wij 是连接可见层神经元 i 和隐藏层神经元 j 的权重。



### 想法

优化算法无非两点：

- 更好的塑造能量景观。
- 更好的通过能量景观来寻求最小值。



在更加深入的了解到能量景观后，我也意识到了能量景观除了人工干预来人为的调整，本身的形状可能无法系统的调整。但是我认为利用能量景观的物理形状是很好的解决寻求最小值的办法。所以我有了以下的想法，但很可惜，基本上想到的都被之前的探索者实现过甚至做的更好。

1.我们可不可以通过多次统计下降到一个局部最小值的能量下降的路线，然后我们找到一个他们路线中的一个“水平面”（从谷底抬升能量，获得），即一个同一个能量值的一个圈它代表着在下降过程中的一个能量值相同的时候，然后向没有路线的方向进行随机探索，找到向下的路？

**注释**：重点在于根据现有解找到一个之前没有的方向进行探索。

类似的算法：遗传算法。

2.我们通过注水，将小球浮起来，脱离最小值，在顺着水的物理性质引导，寻找新的最小值。

**注释**：本质上在于改变能量景观的形状（顺着能量景观，变成顺着水，以水的形状来适应能量景观）

类似的算法：

**Metadynamics (元动力学) 或 SGO (Stochastic Gradient Origin) 算法：**

- 这些算法就是通过在探索过的能量盆地中**动态累积“偏置势能”**，使得这些局部最小值被“填平”或“抬高”。
- 当一个坑被填平后，能量景观的**梯度**在这些区域会发生变化，小球（系统）就会被引导到未被填平的、更低的区域。

**注释**：简单来说偏置势能就是一个参数，可以影响神经元激活或者抑制的倾向。从而改变权重，影响能量景观（能量景观成型依赖于权重）。

**分水岭算法 (Watershed Algorithm) 的优化应用：**

- 在图像处理中，分水岭算法通过模拟注水来分割区域。[图像分割的经典算法：分水岭算法 - 知乎](https://zhuanlan.zhihu.com/p/67741538)

3.通过给小球添加动量概念，突破局部最小值。

**注释**：重点在于，利用能量景观的物理形状和物理性质，在寻求最小值的时候，为其增加惯性，从而突破最小值。

类似的算法：动量优化器。

### 心得

在这周的思考与学习中，我更加深刻的理解到了，物理意义，物理模型在优化算法中的重要性，也理解了科学家在为什么致力于多方向多体系的推进项目。



