---
title: 学习周报第六周
tags:
  - 人工智能
categories:
  - 周报
  
description: 并行计算实验室学习周报
abbrlink: 5f94500d
date: 2025-09-08 10:28:22
---
## 霍普菲尔德网络以及玻尔兹曼机周报

### 霍普菲尔德网络实验

**代码**：

~~~python
import numpy as np
import matplotlib.pyplot as plt

# 设置 Matplotlib 字体以支持中文显示
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS'] # 尝试多个字体
plt.rcParams['axes.unicode_minus'] = False # 解决负号显示问题

# --- 霍普菲尔德网络核心函数 (这些函数内部处理的始终是1D数据) ---

def train(patterns_1d):
    """
    根据给定的1D模式集训练霍普菲尔德网络权重矩阵。
    patterns_1d: 列表，每个元素是一个一维numpy数组（扁平化的模式）。
    """
    num_neurons = patterns_1d[0].shape[0]
    w = np.zeros((num_neurons, num_neurons))
    for pattern in patterns_1d:
        pattern_col = pattern.reshape(-1, 1) # 确保模式是列向量
        w += np.dot(pattern_col, pattern_col.T)
    np.fill_diagonal(w, 0) # 自身连接设置为0
    return w

def E(state_1d, w):

    state_col = state_1d.reshape(-1, 1) # 确保状态是列向量
    e = -0.5 * np.dot(state_col.T, np.dot(w, state_col))
    return e.item() # 返回标量能量值

def update(state_1d, w, index):

    current_state_1d = np.copy(state_1d) # 创建副本以避免直接修改传入的数组
    net_input = np.dot(w[index, :], current_state_1d)
    
    new_state_val = current_state_1d[index] # 默认为不改变
    changed = False

    if net_input > 0:
        new_state_val = 1
    elif net_input < 0:
        new_state_val = -1
    # 如果 net_input 为 0，保持状态不变

    if new_state_val != current_state_1d[index]:
        changed = True
        current_state_1d[index] = new_state_val
        
    return current_state_1d, changed

def work(initial_state_1d, w, n_iterations=1000, max_converge_sweeps=5):
   
    current_state_1d = np.copy(initial_state_1d)
    e_history = [E(initial_state_1d, w)]
    num_neuron = initial_state_1d.shape[0]
    
    no_change_count = 0 
    convergence_threshold = max_converge_sweeps * num_neuron

    for i in range(n_iterations):
        index = np.random.randint(num_neuron) # 随机选择神经元进行更新
        
        current_state_1d, changed = update(current_state_1d, w, index)
        
        e_history.append(E(current_state_1d, w))
        
        if not changed:
            no_change_count += 1
        else:
            no_change_count = 0 

        if no_change_count >= convergence_threshold:
            break # 认为收敛
            
    return current_state_1d, e_history

# --- 辅助函数：处理1D和2D数据转换 ---

def _flatten_pattern(pattern):

    return pattern.flatten()

def _reshape_pattern(pattern_1d, original_shape):

    return pattern_1d.reshape(original_shape)

def create_random_pattern(shape):

    return np.random.choice([-1, 1], size=shape)

def add_noise(pattern, noise_level=0.05):

    original_shape = pattern.shape
    flat_pattern = _flatten_pattern(pattern)
    noisy_flat_pattern = np.copy(flat_pattern)
    
    num_to_flip = int(np.round(len(flat_pattern) * noise_level))
    
    if num_to_flip == 0 and noise_level > 0 and len(flat_pattern) > 0:
        num_to_flip = 1

    if num_to_flip > 0:
        flip_indices = np.random.choice(len(flat_pattern), num_to_flip, replace=False)
        noisy_flat_pattern[flip_indices] *= -1
        
    return _reshape_pattern(noisy_flat_pattern, original_shape)

def get_overlap(state1, state2):

    flat_state1 = _flatten_pattern(state1)
    flat_state2 = _flatten_pattern(state2)
    if len(flat_state1) == 0: return 0
    return np.dot(flat_state1, flat_state2) / len(flat_state1)

# --- 模式存储能力测试实验 ---

def test_storage_capacity(pattern_shape, max_patterns_to_test, num_trials_per_pattern_count=50, noise_level=0.05):

    
    # 确定总神经元数量 (N)
    if isinstance(pattern_shape, int):
        num_neurons = pattern_shape
    elif isinstance(pattern_shape, tuple):
        num_neurons = np.prod(pattern_shape)
    else:
        raise ValueError("pattern_shape 必须是整数（1D）或元组（2D）。")

    # 我们将测试的模式数量范围
    pattern_counts = np.unique(np.linspace(1, max_patterns_to_test, 15).astype(int))
    pattern_counts = pattern_counts[pattern_counts <= num_neurons]
    if len(pattern_counts) == 0:
        print("错误：模式数量范围无效，请检查 pattern_shape 和 max_patterns_to_test 参数。")
        return
    
    success_rates = []
    theoretical_capacity_estimate = 0.138 * num_neurons

    print(f"\n--- 评估霍普菲尔德网络存储能力 ---")
    print(f"模式形状: {pattern_shape}")
    print(f"总神经元数量 (N): {num_neurons}")
    print(f"理论容量参考 (0.138 * N): {theoretical_capacity_estimate:.2f} 个模式")
    print(f"噪声水平: {noise_level * 100:.0f}%")
    print(f"每个模式数量重复试验次数: {num_trials_per_pattern_count}\n")
    print("正在运行测试，请稍候...")

    for p_count in pattern_counts:
        successful_recalls = 0
        
        for _ in range(num_trials_per_pattern_count):
            if p_count == 0: continue
            
        
            stored_patterns_original_shape = [create_random_pattern(pattern_shape) for _ in range(p_count)]
           
            stored_patterns_1d = [_flatten_pattern(p) for p in stored_patterns_original_shape]
            w = train(stored_patterns_1d)

           
            for original_pattern_original_shape in stored_patterns_original_shape:
                
                initial_state_original_shape = add_noise(original_pattern_original_shape, noise_level)
              
                initial_state_1d = _flatten_pattern(initial_state_original_shape)
                
                final_state_1d, _ = work(initial_state_1d, w, n_iterations=num_neurons * 100, max_converge_sweeps=5)
            
                overlap = get_overlap(final_state_1d, _flatten_pattern(original_pattern_original_shape))
                if abs(overlap) > 0.95: 
                    successful_recalls += 1
                
        total_attempts = p_count * num_trials_per_pattern_count
        if total_attempts > 0:
            success_rate = successful_recalls / total_attempts
        else:
            success_rate = 0 
        
        success_rates.append(success_rate)
        print(f"  模式数量 P = {p_count:2d}: 召回成功率 = {success_rate:.3f}")

    # --- 结果可视化 ---
    plt.figure(figsize=(10, 6))
    plt.plot(pattern_counts, success_rates, marker='o', linestyle='-', color='blue')
    
    if theoretical_capacity_estimate > 0:
        plt.axvline(x=theoretical_capacity_estimate, color='red', linestyle='--', 
                    label=f'理论容量参考 (≈{theoretical_capacity_estimate:.1f})')
    
    plt.title('霍普菲尔德网络模式存储能力', fontsize=16)
    plt.xlabel('存储模式数量 (P)', fontsize=12)
    plt.ylabel('平均召回成功率', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.ylim(-0.05, 1.05)
    plt.legend()
    plt.show()

    print("\n评估完成。")

# --- 运行评估 ---
if __name__ == '__main__':
    # --- 参数调控 ---
    # 选择模式的形状
    # 1D 数据: pattern_shape_param = 50 (表示50个神经元)
    # 2D 数据 (例如: 10x10 图像): pattern_shape_param = (10, 10) (总神经元数 N=100)
    pattern_shape_param = 30 #(10, 10) # 示例: 10x10 的2D图像，总共100个神经元
    # pattern_shape_param = 50 # 示例: 50个神经元的1D数据

    # 根据总神经元数量（N）来调整 max_patterns_to_test
    if isinstance(pattern_shape_param, int):
        num_total_neurons = pattern_shape_param
    else:
        num_total_neurons = np.prod(pattern_shape_param)

    max_patterns_to_test_ratio = 0.5 
    max_patterns_to_test = int(num_total_neurons * max_patterns_to_test_ratio)
    if max_patterns_to_test == 0: max_patterns_to_test = 1

    num_trials = 100 
    noise_level_param = 0.05 

    test_storage_capacity(
        pattern_shape=pattern_shape_param,
        max_patterns_to_test=max_patterns_to_test,
        num_trials_per_pattern_count=num_trials,
        noise_level=noise_level_param
    )

    # 存储和显示2D图像
   
    print("\n--- 2D 模式召回演示 ---")
    if isinstance(pattern_shape_param, tuple): # 只有当设置为2D模式时才演示
        demo_shape = pattern_shape_param
        num_demo_neurons = np.prod(demo_shape)
        
        # 存储一个2D模式
        original_2d_pattern = create_random_pattern(demo_shape)
        # 将2D模式扁平化进行训练
        w_demo = train([_flatten_pattern(original_2d_pattern)])

        # 添加噪声并扁平化作为初始状态
        noisy_2d_input = add_noise(original_2d_pattern, noise_level=0.1)
        noisy_1d_input = _flatten_pattern(noisy_2d_input)

        # 运行网络
        recalled_1d_pattern, _ = work(noisy_1d_input, w_demo, n_iterations=num_demo_neurons * 100, max_converge_sweeps=5)
        # 将召回的1D模式重新塑形回2D
        recalled_2d_pattern = _reshape_pattern(recalled_1d_pattern, demo_shape)

        # 显示结果
        fig, axes = plt.subplots(1, 3, figsize=(12, 4))
        axes[0].imshow(original_2d_pattern, cmap='binary')
        axes[0].set_title('原始模式')
        axes[0].axis('off')

        axes[1].imshow(noisy_2d_input, cmap='binary')
        axes[1].set_title('带噪声输入')
        axes[1].axis('off')

        axes[2].imshow(recalled_2d_pattern, cmap='binary')
        axes[2].set_title('召回模式')
        axes[2].axis('off')
        plt.suptitle(f"2D模式召回演示 (重叠度: {get_overlap(original_2d_pattern, recalled_2d_pattern):.3f})")
        plt.tight_layout()
        plt.show()
    else:
        print("当前模式形状为1D，跳过2D模式召回演示。")
~~~



~~~
--- 评估霍普菲尔德网络存储能力 ---
模式形状: 30
总神经元数量 (N): 30
理论容量参考 (0.138 * N): 4.14 个模式
噪声水平: 5%
每个模式数量重复试验次数: 100

正在运行测试，请稍候...
  模式数量 P =  1: 召回成功率 = 1.000
  模式数量 P =  2: 召回成功率 = 0.995
  模式数量 P =  3: 召回成功率 = 0.990
  模式数量 P =  4: 召回成功率 = 0.968
  模式数量 P =  5: 召回成功率 = 0.828
  模式数量 P =  6: 召回成功率 = 0.735
  模式数量 P =  7: 召回成功率 = 0.549
  模式数量 P =  8: 召回成功率 = 0.535
  模式数量 P =  9: 召回成功率 = 0.308
  模式数量 P = 10: 召回成功率 = 0.256
  模式数量 P = 11: 召回成功率 = 0.145
  模式数量 P = 12: 召回成功率 = 0.116
  模式数量 P = 13: 召回成功率 = 0.056
  模式数量 P = 14: 召回成功率 = 0.044
  模式数量 P = 15: 召回成功率 = 0.023
~~~



![image](https://moshiqiqian.github.io/picx-images-hosting/周报/image.6bhf8ds5o7.webp)

**总结**：

1.重新更新代码后，霍普菲尔德网络的存储容量与标准数据吻合，重新查看原本的代码，发现主要是因为最开始写的work函数中选择随机的神经元更新，但是判断收敛的条件不合理与神经元数量少，训练数据单一，这会导致后续实验的时候，降低霍普菲尔德网络的准确性。所以将更新的模式重新优化，最终得出实验结果。在超过存储极限后，召回率断崖式下降。

~~~python
# 原本的代码
def work(initial_state,w,n = 1000):
    current_state = np.copy(initial_state)
    e_history = [E(initial_state,w)]
    num_neuron = initial_state.shape[0]
    
    state_history = [np.copy(initial_state)] 

    for i in range(n):
        index = np.random.randint(num_neuron) 
        current_state,changed = update(current_state,w,index)
        e_history.append(E(current_state, w))
        state_history.append(np.copy(current_state)) 
        
    return current_state, e_history, state_history
~~~

~~~python
# 修改后的代码
def work(initial_state_1d, w, n_iterations=1000, max_converge_sweeps=5):
   
    current_state_1d = np.copy(initial_state_1d)
    e_history = [E(initial_state_1d, w)]
    num_neuron = initial_state_1d.shape[0]
    
    no_change_count = 0 
    convergence_threshold = max_converge_sweeps * num_neuron

    for i in range(n_iterations):
        index = np.random.randint(num_neuron) 
        
        current_state_1d, changed = update(current_state_1d, w, index)
        
        e_history.append(E(current_state_1d, w))
        
        if not changed:
            no_change_count += 1
        else:
            no_change_count = 0 

        if no_change_count >= convergence_threshold: # 根据神经元数量确定收敛的条件
            break 
            
    return current_state_1d, e_history
~~~

### 玻尔兹曼机的相关内容

详细看网站：[玻尔兹曼机 | moshiqiqian](https://www.moshiqiqian.top/post/bb733ff7.html)

### 下周计划

1.进一步学习受限玻尔兹曼机

2.完成受限玻尔兹曼机的代码实现

3.学习QDF ,QDA相关知识

### 问题

1. 正所谓赫布学习不分正反，我了解到对比赫布学习也不分正反。

   在数学上：可以理解为同号相乘必然为正，异号相乘为负，所以神经元的状态相反，那么权重完全一致。

   - 但是在能量景观上，该如何理解，是同一个谷/峰？还是一个完全相反的谷/峰？他会在哪里？整个能量景观的一部分？还是不存在的只是虚构的？

   - 如果是生成了一个现实存在的相反，那么是否可以认为，能量景观是守恒的，一部分在水平面降低了，那么会有另一部分在水平面升高了？

   - 如果是守恒的，那么玻尔兹曼机中的能量景观中的谷是否会比霍普菲尔德网络更陡峭（同样的模式）？

   - 我们接下来提升人工智能的效率，能否致力于极致的拉高非正确的谷/峰，这样正确的能量景观将会很陡峭，然后忽略其中的局部最小值（因为能量景观的变形，他的带来的阻力会很小，我们给小球一个初速度，让他可以用惯性来突破这个局部最小值）。

   