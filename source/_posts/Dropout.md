---
title: Dropout
abbrlink: 5d34d655
date: 2025-09-24 20:11:01
tags:
  - Dropout
categories:
  - 教学
  - 人工智能
  - Dropout
description: Dropout详解
---
## Dropout

### 简介

Dropout是深度学习中为了抑制神经网络过拟合而引入的训练机制。

想要防止过拟合，我们可以用模型组合提升机器学习方法的性能。但是对大型神经网络来说，简单的将多个网路输出进行平均的做法，有很多挑战：

- 要使多个模型真正“不同”，它们需要有不同的架构或者在不同的数据子集上进行训练。然而，为大型网络找到最佳架构参数非常困难，并且训练多个大型网络需要巨大的计算资源和海量数据。
- 即使能够训练多个大型网络，在需要快速响应的实际应用中，同时使用所有模型进行预测是不可行的，因为这会带来巨大的计算开销。

Dropout是一种能解决这两个问题的技术。他既可以防止过拟合，并且提供了一种有效地将许多不同的神经网络架构近似组合在一起的方法。

具体来说：神经网络中的单元（隐藏和可见），通过移除一个神经元（将其以及所有传入传出的连接从网络中暂时删除）。删除的神经元是随机的，每个单元都以独立的固定概率p保留，其中p可以用验证集进行保留，也可以简单的设置为0.5，这个值一般来说对所有任务和网络来说是最佳的。对于输入单元通常保留概率更接近1而不是0.5。

### 流程

- **作用时间：** 在标准前向传播中加入 **Dropout** 机制来防止过拟合。
- **前向传播过程：**
  - r∼Bernoulli(p)：引入一个随机向量 r，它的每个元素都服从伯努利分布，概率为 p。
  - y=r∗y：对上一层的输出 y 进行按元素相乘，随机地将一些神经元的输出置为0（即“丢弃”它们）。
  - z<sub>i</sub>=w<sub>i</sub>y+b<sub>i</sub>：将丢弃后的输出 y~ 作为新的输入，进行前向传播计算。
  - y<sub>i</sub>=f(z<sub>i</sub>)：同样，通过激活函数得到最终输出。

**注意：**Dropout要对剩余神经元进行放大输出，主要是为了保持输出值整体期望的稳定性因为期望上只保留`p*100%`的神经元，这会导致传输到下一层的值在期望上只有原来的`p*100%`。因此，先对剩下的`p*100%`个神经元扩大1/p倍再输出，这样可以保持传到下一层的值在期望上是保持一致的。

### 问题

1、Dropout现在绝大多数是作用于隐藏层的，但在论文中提到了，输入层也可以用Dropout，只不过保留概率接近于一。我认为可以倾向于说，本质上来说Dropout在隐藏输入层的效果并不是很好，或者说只是对于特殊的情况才会有正向效果。

换一种解释来说，在训练的时候，断开隐藏层的神经元，本身就是与输入层的神经元断开链接的，这是否在非全连接的情况下，意味着输入层的神经元有概率发生所有的链接都断开，看起来是让Dropout隐藏起来了一样。



2、伯努利分布（Bernoulli distribution）是概率论中最简单的一种离散型概率分布。它描述的是一个只有两种可能结果的单次随机试验，比如抛硬币。这两种结果通常用 1 和 0 来表示，它们发生的概率是固定的：

- **结果 1（成功）：** 发生的概率为 p。
- **结果 0（失败）：** 发生的概率为 1−p。

在Dropout中作用于判断是否保留神经元的方式。

3、为什么Dropout可以避免过拟合。

**训练时：** 它通过随机关闭神经元，强制每个神经元都学习独立且通用的特征，打破了神经元之间的过度依赖。

**测试时：** 它实际上是在利用一种“模型集成”的平均化思想，将多个“瘦身版”网络的预测结果融合，从而得到更稳健、更准确的最终结果。